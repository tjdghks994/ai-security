{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Choejaegeon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choe1842/ai-security/blob/master/Choejaegun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQ37XxyaXm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient Descent 구현 하기\n",
        "\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [3.0, 6.0, 9.0]\n",
        "\n",
        "w = 1 # 임의의 값 지정\n",
        "\n",
        "def forward(x): \n",
        "  return x * w # 기본 식\n",
        "\n",
        "def loss(x,y): # loss 계산\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y) # 예측 값과 실제 값의 거리 차를 돌려줌\n",
        "\n",
        "def gradient(x,y): \n",
        "  return 2 * x * (x * w - y) # gradient를 계산함\n",
        "\n",
        "print(\"학습 전\", \"값 :\", forward(4))\n",
        "\n",
        "for epoch in range(15): # 학습 과정\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    \n",
        "    w = w - 0.01 * grad # 가중치 업데이트\n",
        "    \n",
        "    print(\"\\tgrad:\", x_val, y_val, round(grad,2))\n",
        "    l = loss(x_val, y_val)\n",
        "   \n",
        "  print(\"prograss:\", epoch, \"w = \", round(w,2), \"loss = \", round(l,5))\n",
        "  \n",
        " \n",
        "print(\"학습완료\", \"값 :\", forward(4)) # 학습 확인\n",
        "\n",
        "\n",
        "# Advanced Convolutional neural network 구현하기\n",
        "\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn # 딥러닝 모델에 필요한 모듈이 모아져있는 패키지\n",
        "import torch.nn.functional as F # 함수의 input으로 반드시 연산되어야 하는 값을 받는 기능\n",
        "import torch.optim as optim # 최적화 방법이 있는 패키지\n",
        "from torchvision import datasets, transforms # torch에서 제공해주는 데이터셋 가져오기\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # gpu 사용 가능 확인\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',train = True, transform = transforms.ToTensor(),download = True) # 학습용 데이터셋\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',train = False, transform = transforms.ToTensor()) # 학습 정도를 테스트하기 위한 데이터셋\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)  # 학습 데이터를 받아오는 함수\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False) # 테스트 데이터를 받아오는 함수\n",
        "\n",
        "class InceptionModule(nn.Module): # 여러 필터의 결과를 합치기 위한 인셉션 모듈 구현, nn.Module 상속\n",
        "\n",
        "  def __init__(self,in_channels): # 네트워크 계층 정의\n",
        "    super(InceptionModule,self).__init__() \n",
        "    self.branch1x1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 : 연산을 줄이면서 한 레이어에서 더 깊은 논리 처리 가능\n",
        "  \n",
        "    self.branch5x5_1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 과 같은 기능 \n",
        "    self.branch5x5_2 = nn.Conv2d(16,24,kernel_size=5,padding=2) # 16채널을 5X5의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "  \n",
        "    self.branch3x3_1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 과 같은 기능\n",
        "    self.branch3x3_2 = nn.Conv2d(16,24,kernel_size=3,padding=1) # 16채널을 3X3의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "    self.branch3x3_3 = nn.Conv2d(24,24,kernel_size=3,padding=1) # 24채널을 3X3의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "  \n",
        "    self.branch_pool = nn.Conv2d(in_channels,24,kernel_size=1) \n",
        "\n",
        "  def forward(self,x): # 하나의 x를 4개로 쪼개주기 위함\n",
        "    \n",
        "    branch1x1 = self.branch1x1(x) \n",
        "    \n",
        "    branch5x5 = self.branch5x5_1(x) \n",
        "    branch5x5 = self.branch5x5_2(branch5x5) \n",
        "    \n",
        "    branch3x3 = self.branch3x3_1(x) \n",
        "    branch3x3 = self.branch3x3_2(branch3x3) \n",
        "    branch3x3 = self.branch3x3_3(branch3x3) \n",
        "    \n",
        "    branch_pool = F.avg_pool2d(x,kernel_size=3,stride=1,padding=1) \n",
        "    branch_pool = self.branch_pool(branch_pool) \n",
        "    \n",
        "    outputs = [branch1x1,branch5x5,branch3x3,branch_pool]\n",
        "    return torch.cat(outputs, 1) # 4개의 함수를 하나로 합쳐준다. 채널의 개수는 총 88개(16+24+24+24)\n",
        "  \n",
        "  \n",
        "  \n",
        "class MainNet(nn.Module): # 전체 모델 구성\n",
        "  def __init__(self): \n",
        "    super(MainNet,self).__init__() \n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size=5)  # 5x5의 Conv layer 10채널 통과\n",
        "    \n",
        "    self.conv2 = nn.Conv2d(88,20,kernel_size=5) # # 5x5의 Conv layer 20채널 통과, 인셉션 모델의 채널의 개수가 총 88개 이기 때문에 채널을 88개로 함\n",
        "    \n",
        "    self.incept1 = InceptionModule(in_channels=10) # 인셉션 모듈 통과\n",
        "    self.incept2 = InceptionModule(in_channels=20) # 인셉션 모듈 통과\n",
        "    \n",
        "    self.max_pool = nn.MaxPool2d(2)  # 주어진 필터 범위 내에서 최대값을 뽑아내는 과정\n",
        "    self.fc = nn.Linear(1408,10) # 마지막 Conv layer의 cell을 한줄로 나열한 후 그 개수에 맞추어 Fully connected layer의 input으로 넘겨주는 것, 즉, 1408개의 데이터를 10개로 바꿈\n",
        "\n",
        "  def forward(self,x): \n",
        "    in_size = x.size(0) # size(0)을 하면 (n, 28*28)중 n을 리턴함 \n",
        "    x = F.relu(self.max_pool(self.conv1(x))) # 학습을 위한 활성화 함수 relu\n",
        "    x = self.incept1(x) \n",
        "    x = F.relu(self.max_pool(self.conv2(x))) \n",
        "    x = self.incept2(x) \n",
        "    x = x.view(in_size,-1) \n",
        "    x = self.fc(x) \n",
        "    return F.log_softmax(x) # 아웃풋을 한번에 묶어 결과를 조정해주는 소프트맥스 함수 사용\n",
        "\n",
        "model = MainNet().to(device) # 모델 정의\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5) # 가중치를 업데이트 시켜주는 최적화 (Stochastic Gradient Descent) 사용, 과도한 연산 방지\n",
        "criterion = nn.NLLLoss() # loss 함수 정의, softmax함수를 사전에 사용했기 때문에, NLLLoss를 사용합니다.\n",
        "\n",
        "def train(epoch): \n",
        "  model.train() \n",
        "  for batch_idx, (data, target) in enumerate(train_loader): \n",
        "    data = data.to(device) \n",
        "    target = target.to(device) \n",
        "    \n",
        "    output = model(data) # 데이터를 학습 모델에 투입\n",
        "    \n",
        "    optimizer.zero_grad() # gradient buffers를 0으로\n",
        "    loss = criterion(output, target) # loss 계산\n",
        "    loss.backward()  # 역전파 과정을 통해 각 변수마다 loss에 대한 gradient를 구해줌\n",
        "    optimizer.step() # model의 파라미터들을 업데이트 해주는 함수\n",
        "    if batch_idx % 50 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data))\n",
        "\n",
        "#batch: 전체 데이터를 나눠 일부를 묶은 것을 의미함\n",
        "#epoch: 모든 학습 데이터에 대해 forward 와 backward pass를 한번 진행한 상태\n",
        "\n",
        "def test(): # 과학습 가능성을 고려하여 별도의 테스트 함수 선언\n",
        "  model.eval() \n",
        "  test_loss = 0 \n",
        "  correct = 0 \n",
        "  for data, target in test_loader: # 테스트 데이터 사용\n",
        "    data = data.to(device) \n",
        "    target = target.to(device) \n",
        "    output = model(data) \n",
        "    test_loss += criterion(output, target).data # loss 계산\n",
        "    pred = output.data.max(1, keepdim=True)[1] \n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum() #pred배열과 data의 일치 여부 검사해주는 기능\n",
        "   \n",
        "  test_loss /= len(test_loader.dataset)/batch_size \n",
        "  print('\\nTest set: 평균 loss: {:.4f}, 정확도: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "for epoch in range(1, 10): # 최종 실행\n",
        "  train(epoch)\n",
        "  test()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}